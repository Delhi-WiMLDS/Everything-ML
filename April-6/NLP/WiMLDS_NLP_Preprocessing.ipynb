{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WiMLDS_NLP_Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9KES27e6AERU",
        "colab_type": "code",
        "outputId": "cb121101-bb83-46ba-8a47-d50ae47d1c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VIIP9WY1AURm",
        "colab_type": "code",
        "outputId": "ddc77061-42ae-448c-b697-b6b42773840f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "ADSt02gNPV4f",
        "colab_type": "code",
        "outputId": "72dc4c28-aa5e-47b0-f416-8b9a48c73360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Lowercasing\n",
        "input_str = \"Let's try out NLP.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "let's try out nlp.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ouMIqny0Vu5F",
        "colab_type": "code",
        "outputId": "819cf3ab-9b78-4b3a-d277-cdb40ba6138a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "input_str = \"I am a girl who loves to code and is extremely passionate about transforming the world around her through it.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(input_str)\n",
        "result = [i for i in tokens if not i in stop_words]\n",
        "print (result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'girl', 'loves', 'code', 'extremely', 'passionate', 'transforming', 'world', 'around', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Lb4YN2LZQf9",
        "colab_type": "code",
        "outputId": "423d3fec-1b6a-4067-baf1-cfc52b349248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer= PorterStemmer()\n",
        "\n",
        "input_str = \"I am a girl who loves to code and is extremely passionate about transforming the world around her through it.\"\n",
        "input_str=word_tokenize(input_str)\n",
        "\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word),end=\" \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am a girl who love to code and is extrem passion about transform the world around her through it . "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9iF9tGE2bKQ0",
        "colab_type": "code",
        "outputId": "e2123c38-1a35-40ea-9d55-7cd8bad4dffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "input_str = \"I am a girl who loves to code and is extremely passionate about transforming the world around her through it.\"\n",
        "input_str=word_tokenize(input_str)\n",
        "\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word), end=\" \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am a girl who love to code and is extremely passionate about transforming the world around her through it . "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IM7lMDyddhLK",
        "colab_type": "code",
        "outputId": "3dcff07b-118c-4217-d427-1760c97d6f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#initialisation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "sample_text = [\"One of the most basic ways we can numerically represent words is through the one-hot encoding method (also sometimes called count vectorizing).\"]\n",
        "\n",
        "# calling vectoriser on the given text\n",
        "vectorizer.fit(sample_text)\n",
        "\n",
        "# This will print out a list of words used, and their index in the vectors\n",
        "print('Vocabulary: ')\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# If we would like to actually create a vector, we can do so by passing the\n",
        "# text into the vectorizer to get back counts\n",
        "vector = vectorizer.transform(sample_text)\n",
        "\n",
        "# Our final vector:\n",
        "print('Full vector: ')\n",
        "print(vector.toarray())\n",
        "\n",
        "# Or if we wanted to get the vector for one word:\n",
        "print('Basic vector: ')\n",
        "print(vectorizer.transform(['basic']).toarray())\n",
        "\n",
        "# We could also do the whole thing at once with the fit_transform method:\n",
        "print('One swoop:')\n",
        "new_text = ['Today is the day that I do the thing today, today']\n",
        "new_vectorizer = CountVectorizer()\n",
        "print(new_vectorizer.fit_transform(new_text).toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: \n",
            "{'one': 12, 'of': 11, 'the': 15, 'most': 9, 'basic': 1, 'ways': 18, 'we': 19, 'can': 3, 'numerically': 10, 'represent': 13, 'words': 20, 'is': 7, 'through': 16, 'hot': 6, 'encoding': 5, 'method': 8, 'also': 0, 'sometimes': 14, 'called': 2, 'count': 4, 'vectorizing': 17}\n",
            "Full vector: \n",
            "[[1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1]]\n",
            "Basic vector: \n",
            "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "One swoop:\n",
            "[[1 1 1 1 2 1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}